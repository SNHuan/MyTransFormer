# MyTransformer

ä¸€ä¸ªåŸºäº Transformer æ¶æ„çš„ä¸­æ–‡è¯­è¨€æ¨¡å‹å®ç°ã€‚

## é¡¹ç›®ç»“æ„

- `modeling/`: æ¨¡å‹æ ¸å¿ƒå®ç°
  - `modeling.py`: åŒ…å« Transformer æ¨¡å‹çš„æ ¸å¿ƒå®ç°ï¼ŒåŒ…æ‹¬è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€MLPã€è§£ç å™¨å±‚ç­‰
  - `config.py`: æ¨¡å‹é…ç½®ç±»ï¼Œå®šä¹‰äº†æ¨¡å‹çš„è¶…å‚æ•°

- `custom_train.py`: æ¨¡å‹è®­ç»ƒè„šæœ¬ï¼Œæ”¯æŒä»å¤´å¼€å§‹è®­ç»ƒ
- `continue_train.py`: ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒçš„è„šæœ¬
- `better_generate.py`: æ–‡æœ¬ç”Ÿæˆè„šæœ¬ï¼Œç”¨äºä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆæ–‡æœ¬

## åŠŸèƒ½ç‰¹ç‚¹

- å®ç°äº†å®Œæ•´çš„ Transformer è§£ç å™¨æ¶æ„
- æ”¯æŒ RMSNorm å½’ä¸€åŒ–
- å®ç°äº† SwiGLU æ¿€æ´»å‡½æ•°
- æ”¯æŒ KV ç¼“å­˜ä»¥åŠ é€Ÿæ¨ç†
- æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ
- ä¸ ğŸ¤— Transformers åº“å®Œå…¨å…¼å®¹

## ä½¿ç”¨æ–¹æ³•

### è®­ç»ƒæ¨¡å‹

```bash
python custom_train.py \
    --model_size nano \
    --max_steps 100 \
    --batch_size 1 \
    --gradient_accumulation_steps 16 \
    --max_length 32 \
    --fp16 \
    --tokenizer_path ./models/new_tokenizer \
    --dataset_path data/chinese_deepseek_r1
```

### ç»§ç»­è®­ç»ƒ

```bash
python continue_train.py \
    --checkpoint_path ./output/checkpoint-1000 \
    --tokenizer_path ./models/new_tokenizer \
    --max_steps 1000 \
    --batch_size 1 \
    --gradient_accumulation_steps 16 \
    --max_length 32 \
    --fp16
```

### ç”Ÿæˆæ–‡æœ¬

```bash
python better_generate.py \
    --model_path ./output/checkpoint-10000 \
    --tokenizer_path models/new_tokenizer \
    --prompt "ä½ å¥½" \
    --max_length 50 \
    --temperature 0.7 \
    --top_p 0.9 \
    --top_k 100
```

## æ¨¡å‹é…ç½®

æ”¯æŒä¸¤ç§é¢„è®¾é…ç½®ï¼š

- nano:
  - hidden_size: 256
  - num_hidden_layers: 4
  - num_attention_heads: 4
  - intermediate_size: 1024

- medium:
  - hidden_size: 768
  - num_hidden_layers: 12
  - num_attention_heads: 12
  - intermediate_size: 3072

## ä¾èµ–

- PyTorch
- Transformers
- Datasets
- NumPy
